(window.webpackJsonp=window.webpackJsonp||[]).push([[17],{648:function(t,a,e){"use strict";e.r(a);var n=e(14),v=Object(n.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"李宏毅机器学习视频课程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#李宏毅机器学习视频课程"}},[t._v("#")]),t._v(" 李宏毅机器学习视频课程")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.bilibili.com/video/BV1Wv411h7kN?p=34",target:"_blank",rel:"noopener noreferrer"}},[t._v("b站链接"),e("OutboundLink")],1),t._v("\n[]# chapter2")]),t._v(" "),e("p",[t._v("第二节 机器学习任务攻略 P10 - 14:49")]),t._v(" "),e("p",[t._v("模型训练出来，的验证集效果不好。")]),t._v(" "),e("p",[t._v("第一，首先考虑的是模型本身， 是不是model bias， 也即模型（结构）不够合适。（function set 太小）")]),t._v(" "),e("p",[t._v("第二，或者是优化问题， 即模型不能优化到更低的function set。（即便存在）")]),t._v(" "),e("p",[t._v("我们可以用一些简单的模型，比如svm。 这些模型往往能保证 有一个很低的loss。")]),t._v(" "),e("p",[t._v("如果深度模型的loss。比这些高。")]),t._v(" "),e("p",[t._v("那就要考虑从以上两个角度去优化。")]),t._v(" "),e("p",[t._v("类神经网络训练不起来怎么办(二) 批次 (batch... P12 - 14:17")]),t._v(" "),e("p",[t._v("由于在GPU上运算， batch越大其实一个epoch的时间是会更短的。")]),t._v(" "),e("p",[t._v("但是batch越大， optimization的效果就会越差。因此在validation上的准确率没有小batch好。")]),t._v(" "),e("p",[t._v("类神经网络训练不起来怎么办(二) 批次 (batch... P12 - 18:27")]),t._v(" "),e("p",[t._v("Flat Minima  和Sharp Minima。")]),t._v(" "),e("p",[t._v("我们认为flat的minima，更容易在Validation上得到更好的结果。")]),t._v(" "),e("p",[t._v("大小Batch的总结")]),t._v(" "),e("p",[t._v("当然也有针对大batch的优化方法。")]),t._v(" "),e("p",[t._v("类神经网络训练不起来怎么办(二) 批次 (batch... P12 - 22:38")]),t._v(" "),e("h2",{attrs:{id:"adam梯度下降。-动态的-learning-rate-momentum。"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#adam梯度下降。-动态的-learning-rate-momentum。"}},[t._v("#")]),t._v(" Adam梯度下降。 动态的 Learning rate + momentum。")]),t._v(" "),e("p",[t._v("类神经网络训练不起来怎么办(三) 自动调整学习率... P13 - 23:00")]),t._v(" "),e("h2",{attrs:{id:"learning-rate-schedual。-decay-or-warm-up"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#learning-rate-schedual。-decay-or-warm-up"}},[t._v("#")]),t._v(" learning rate schedual。 decay or warm up")]),t._v(" "),e("p",[t._v("类神经网络训练不起来怎么办(三) 自动调整学习率... P13 - 29:31")]),t._v(" "),e("p",[t._v("warm up 类似TCP的快重启。")]),t._v(" "),e("h2",{attrs:{id:"总结"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),e("p",[t._v("类神经网络训练不起来怎么办(三) 自动调整学习率... P13 - 36:20")]),t._v(" "),e("h1",{attrs:{id:"batch-normalization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#batch-normalization"}},[t._v("#")]),t._v(" batch_normalization.")]),t._v(" "),e("p",[t._v("为什么需要做正则")]),t._v(" "),e("h1",{attrs:{id:"self-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#self-attention"}},[t._v("#")]),t._v(" self-attention")]),t._v(" "),e("p",[t._v("一个比较经典的任务是 seq2seq 的任务即输入和输出序列的长度是一样的。（例如， 对于一段句子的词性标注, Sequence Labeling， 标记一段句子中一个）")]),t._v(" "),e("p",[t._v("语音辨识的时候， 由于向量序列的长度L非常大。 而在计算attention的时候会有L*L复杂度的计算量。所以我们常常会做trunk(也就是每个位置的attention矩阵只考虑window_size的大小）")]),t._v(" "),e("p",[t._v("On the Relationship between Self-Attention and Convolutional Layers 用数学方法证明了其实CNN就是Self-Attention的一个子集")]),t._v(" "),e("p",[t._v("self-A vs RNN.")]),t._v(" "),e("p",[t._v("RNN 是一个记忆网络， 把上一步的输出作为隐藏层输入到下一个向量的处理中。")]),t._v(" "),e("p",[t._v("RNN 是基于顺序的， 会受到方向和顺序的影响，不能并行。")]),t._v(" "),e("p",[t._v("而self-attention则可以做到这一点。")]),t._v(" "),e("p",[t._v("self-attiontion也可以完全")]),t._v(" "),e("p",[t._v("这就说明了 自注意力的出现self-attention is all your need.")]),t._v(" "),e("h1",{attrs:{id:"transformer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer"}},[t._v("#")]),t._v(" Transformer")]),t._v(" "),e("p",[t._v("Seq2Seq")]),t._v(" "),e("p",[t._v("很多NLP问题都可以强行 用Seq2Seq Model train起来。")])])}),[],!1,null,null,null);a.default=v.exports}}]);